---

title: Distributed and parallel training


keywords: fastai
sidebar: home_sidebar

summary: "Callbacks and helper functions to train in parallel or use distributed training"
description: "Callbacks and helper functions to train in parallel or use distributed training"
nb_path: "nbs/20a_distributed.ipynb"
---
<!--

#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: nbs/20a_distributed.ipynb
# command to build the docs after a change: nbdev_build_docs

-->

<div class="container" id="notebook-container">
        
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>When using multiple GPUs, you will most probably want to fit using distributed training. See <a href="https://github.com/fastai/fastai/blob/master/nbs/examples/distrib.py">examples/distrib.py</a> for a complete example. To use distributed training, there are only two required steps:</p>
<ol>
<li>Add <code>with learn.distrib_ctx():</code> before your <code>learn.fit</code> call</li>
<li>Run your training script with <code>python -m fastai.launch scriptname.py ...args...</code></li>
</ol>
<p>After <code>fastai.launch</code> you can add <code>--gpus 0,1</code> for instance, to use only using GPUs 1 and 2.</p>
<p>If you're using <a href="/data.external.html#untar_data"><code>untar_data</code></a>, or may be downloading or uncompressing data or models as part of your script, you should wrap that code with <a href="/distributed.html#rank0_first"><code>rank0_first</code></a>, which forces that step to occur first just once on the master process, prior to the remaining processes running it in parallel. E.g. instead of:</p>
<div class="highlight"><pre><span></span><span class="n">path</span> <span class="o">=</span> <span class="n">untar_data</span><span class="p">(</span><span class="n">URLs</span><span class="o">.</span><span class="n">IMAGEWOOF_320</span><span class="p">)</span>
</pre></div>
<p>...you instead use:</p>
<div class="highlight"><pre><span></span><span class="n">path</span> <span class="o">=</span> <span class="n">rank0_first</span><span class="p">(</span><span class="n">untar_data</span><span class="p">,</span> <span class="n">URLs</span><span class="o">.</span><span class="n">IMAGEWOOF_320</span><span class="p">)</span>
</pre></div>
<p>See below for details on the full API and underlying helper functions, if needed -- however, note that you will not need anything except the above unless you need to change how the distributed training is implemented.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Parallel">Parallel<a class="anchor-link" href="#Parallel"> </a></h2>
</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="DataParallel.reset" class="doc_header"><code>DataParallel.reset</code><a href="https://github.com/fastai/fastai/tree/master/fastai/distributed.py#L12" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>DataParallel.reset</code>()</p>
</blockquote>
<p>Patch required <code>reset</code> call into <code>DataParallel</code></p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="ParallelTrainer" class="doc_header"><code>class</code> <code>ParallelTrainer</code><a href="https://github.com/fastai/fastai/tree/master/fastai/distributed.py#L18" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>ParallelTrainer</code>(<strong><code>device_ids</code></strong>) :: <a href="/callback.core.html#Callback"><code>Callback</code></a></p>
</blockquote>
<p>Wrap a model <code>DataParallel</code> automatically</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="Learner.to_parallel" class="doc_header"><code>Learner.to_parallel</code><a href="https://github.com/fastai/fastai/tree/master/fastai/distributed.py#L26" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>Learner.to_parallel</code>(<strong><code>device_ids</code></strong>=<em><code>None</code></em>)</p>
</blockquote>
<p>Add <a href="/distributed.html#ParallelTrainer"><code>ParallelTrainer</code></a> callback to a <a href="/learner.html#Learner"><code>Learner</code></a></p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="Learner.detach_parallel" class="doc_header"><code>Learner.detach_parallel</code><a href="https://github.com/fastai/fastai/tree/master/fastai/distributed.py#L33" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>Learner.detach_parallel</code>()</p>
</blockquote>
<p>Remove <a href="/distributed.html#ParallelTrainer"><code>ParallelTrainer</code></a> callback from a Learner</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="Learner.parallel_ctx" class="doc_header"><code>Learner.parallel_ctx</code><a href="https://github.com/fastai/fastai/tree/master/fastai/distributed.py#L40" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>Learner.parallel_ctx</code>(<strong><code>device_ids</code></strong>=<em><code>None</code></em>)</p>
</blockquote>
<p>A context manager to adapt a learner to train in data parallel mode.</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Distributed">Distributed<a class="anchor-link" href="#Distributed"> </a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Helper-functions">Helper functions<a class="anchor-link" href="#Helper-functions"> </a></h3>
</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="DistributedDataParallel.reset" class="doc_header"><code>DistributedDataParallel.reset</code><a href="https://github.com/fastai/fastai/tree/master/fastai/distributed.py#L50" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>DistributedDataParallel.reset</code>()</p>
</blockquote>
<p>Patch required <code>reset</code> call into <code>DistributedDataParallel</code></p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="setup_distrib" class="doc_header"><code>setup_distrib</code><a href="https://github.com/fastai/fastai/tree/master/fastai/distributed.py#L56" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>setup_distrib</code>(<strong><code>gpu</code></strong>=<em><code>None</code></em>)</p>
</blockquote>
<p>Setup this process to participate in distributed training</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="teardown_distrib" class="doc_header"><code>teardown_distrib</code><a href="https://github.com/fastai/fastai/tree/master/fastai/distributed.py#L65" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>teardown_distrib</code>()</p>
</blockquote>
<p>Free distributed training resources</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="DataLoader">DataLoader<a class="anchor-link" href="#DataLoader"> </a></h3>
</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="DistributedDL" class="doc_header"><code>class</code> <code>DistributedDL</code><a href="https://github.com/fastai/fastai/tree/master/fastai/distributed.py#L73" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>DistributedDL</code>(<strong><code>dl</code></strong>, <strong><code>rank</code></strong>=<em><code>None</code></em>, <strong><code>world_size</code></strong>=<em><code>None</code></em>) :: <a href="/data.core.html#TfmdDL"><code>TfmdDL</code></a></p>
</blockquote>
<p>A <a href="/data.core.html#TfmdDL"><code>TfmdDL</code></a> which splits a batch into equal size pieces for each worker</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">dl</span> <span class="o">=</span> <span class="n">TfmdDL</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">50</span><span class="p">)),</span> <span class="n">bs</span><span class="o">=</span><span class="mi">12</span><span class="p">,</span> <span class="n">num_workers</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">4</span><span class="p">):</span>
    <span class="n">dl1</span> <span class="o">=</span> <span class="n">DistributedDL</span><span class="p">(</span><span class="n">dl</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
    <span class="n">test_eq</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">dl1</span><span class="p">),</span> <span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">i</span><span class="o">*</span><span class="mi">13</span><span class="p">,</span> <span class="n">i</span><span class="o">*</span><span class="mi">13</span><span class="o">+</span><span class="mi">12</span><span class="p">)</span><span class="o">%</span><span class="k">50</span>,torch.tensor([i*13+12])%50))
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="DistributedTrainer" class="doc_header"><code>class</code> <code>DistributedTrainer</code><a href="https://github.com/fastai/fastai/tree/master/fastai/distributed.py#L124" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>DistributedTrainer</code>(<strong><code>cuda_id</code></strong>=<em><code>0</code></em>, <strong><code>sync_bn</code></strong>=<em><code>True</code></em>) :: <a href="/callback.core.html#Callback"><code>Callback</code></a></p>
</blockquote>
<p>Wrap <code>model</code> in <code>DistributedDataParallel</code> and <code>dls</code> in <a href="/distributed.html#DistributedDL"><code>DistributedDL</code></a></p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="Learner.to_distributed" class="doc_header"><code>Learner.to_distributed</code><a href="https://github.com/fastai/fastai/tree/master/fastai/distributed.py#L143" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>Learner.to_distributed</code>(<strong><code>cuda_id</code></strong>, <strong><code>sync_bn</code></strong>=<em><code>True</code></em>)</p>
</blockquote>
<p>Add <a href="/distributed.html#DistributedTrainer"><code>DistributedTrainer</code></a> to a learner</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="Learner.detach_distributed" class="doc_header"><code>Learner.detach_distributed</code><a href="https://github.com/fastai/fastai/tree/master/fastai/distributed.py#L151" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>Learner.detach_distributed</code>()</p>
</blockquote>
<p>Remove <a href="/distributed.html#DistributedTrainer"><code>DistributedTrainer</code></a> from a learner</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="distrib_ctx-context-manager"><code>distrib_ctx</code> context manager<a class="anchor-link" href="#distrib_ctx-context-manager"> </a></h3>
</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="Learner.distrib_ctx" class="doc_header"><code>Learner.distrib_ctx</code><a href="https://github.com/fastai/fastai/tree/master/fastai/distributed.py#L160" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>Learner.distrib_ctx</code>(<strong><code>cuda_id</code></strong>=<em><code>None</code></em>, <strong><code>sync_bn</code></strong>=<em><code>True</code></em>)</p>
</blockquote>
<p>A context manager to adapt a learner to train in distributed data parallel mode.</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><code>distrib_ctx</code> prepares a learner to train in distributed data parallel mode.  It assumes these <a href="https://pytorch.org/tutorials/intermediate/dist_tuto.html#initialization-methods">environment variables</a> have all been setup properly, such as those launched by <code>python -m fastai.launch</code>.</p>
<p>Typical usage:</p>

<pre><code>with learn.distrib_ctx(): learn.fit(.....)</code></pre>
<p>It attaches a <a href="/distributed.html#DistributedTrainer"><code>DistributedTrainer</code></a> callback and <a href="/distributed.html#DistributedDL"><code>DistributedDL</code></a> data loader to  the learner, then executes <code>learn.fit(.....)</code>.  Upon exiting the context, it removes the <a href="/distributed.html#DistributedTrainer"><code>DistributedTrainer</code></a> and <a href="/distributed.html#DistributedDL"><code>DistributedDL</code></a>, and destroys any locally created distributed process group.  The process is still attached to the GPU though.</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="rank0_first" class="doc_header"><code>rank0_first</code><a href="https://github.com/fastai/fastai/tree/master/fastai/distributed.py#L179" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>rank0_first</code>(<strong><code>func</code></strong>, <strong>*<code>args</code></strong>, <strong>**<code>kwargs</code></strong>)</p>
</blockquote>
<p>Execute <code>func</code> in the Rank-0 process first, then in other ranks in parallel.</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><a href="/distributed.html#rank0_first"><code>rank0_first</code></a> calls <code>f()</code> in rank-0 process first, then in parallel on the rest, in distributed training mode. In single process, non-distributed training mode, <code>f()</code> is called only once as expected.</p>
<p>One application of <code>rank0_first()</code> is to make fresh downloads via <a href="/data.external.html#untar_data"><code>untar_data</code></a> safe in distributed training scripts launched by <code>python -m fastai.launch &lt;script&gt;</code>:</p>
<p><code>path = untar_data(URLs.IMDB)</code></p>
<p>becomes:</p>
<p><code>path = rank0_first(lambda: untar_data(URLs.IMDB))</code></p>
<p>Some learner factory methods may use <a href="/data.external.html#untar_data"><code>untar_data</code></a> to download pretrained models:</p>
<p><code>learn = text_classifier_learner(dls, AWD_LSTM, drop_mult=0.5, metrics=accuracy)</code></p>
<p>becomes:</p>
<p><code>learn = rank0_first(lambda: text_classifier_learner(dls, AWD_LSTM, drop_mult=0.5, metrics=accuracy))</code></p>
<p>Otherwise, multiple processes will download at the same time and corrupt the data.</p>

</div>
</div>
</div>
</div>
 

